{
  "NameWiz": "NameWiz",
  "Choose AI model": "Choose AI model",
  "Generate creative names for your startup, business, or project!": "Generate creative names for your startup, business, or project!",
  "Rate limit remaining requests and tokens are:": "Rate limit remaining requests and tokens are:",
  "not calculated yet": "not calculated yet",
  "Not Allowed": "Not Allowed",
  "and": "and",
  "üìù Enter one or more keywords (e.g., \"table fashion cats\")": "üìù Enter one or more keywords (e.g., \"Table RPG fashion cats\")",
  "English": "English",
  "Russian": "Russian",
  "Hindi": "Hindi",
  "Professional": "Professional",
  "Fun/Playful": "Fun/Playful",
  "Modern/Minimalist": "Modern/Minimalist",
  "Luxury/High-End": "Luxury/High-End",
  "Generating...": "Generating...",
  "Generate Names": "Generate Names",
  "Generated Names": "Generated Names",
  "Share on Twitter": "Share on Twitter",
  "Share on LinkedIn": "Share on LinkedIn",
  "Share on Telegram": "Share on Telegram",
  "General": "General",
  "Tech Startup": "Tech Startup",
  "Fashion": "Fashion",
  "Food": "Food",
  "Eco-Friendly": "Eco-Friendly",
  "Finance": "Finance",
  "Health": "Health",
  "Education": "Education",
  "Travel": "Travel",
  "Gaming": "Gaming",
  "Blog": "Blog",
  "E-Commerce": "E-Commerce",
  "Real Estate": "Real Estate",
  "Art & Design": "Art & Design",
  "Social Media": "Social Media",
  "Sports & Fitness": "Sports & Fitness",
  "Non-Profit": "Non-Profit",
  "Entertainment": "Entertainment",
  "Photography": "Photography",
  "Technology Services": "Technology Services",
  "Generation process was canceled": "Generation process was canceled",
  "Generating... press again to cancel": "Working on... press again to cancel",
  "Number of names": "Number of names",
  "Temperature value": "Temperature value",
  "Yes": "Yes",
  "No": "No",
  "Cancel": "Cancel",
  "Drop the image here ...": "Drop the image here ...",
  "Drag &apos;n&apos; drop an image here, or click to select image": "Drag'n'drop an image here, or click to select image",
  "Are you sure to cancel?": "Are you sure to cancel?",
  "card_gpt4o_description": "gpt-4o offers a shift in how AI models interact with multimodal inputs. By seamlessly combining text, images, and audio, gpt-4o provides a richer, more engaging user experience.Matching the intelligence of gpt-4 turbo, it is remarkably more efficient, delivering text at twice the speed and at half the cost. Additionally, GPT-4o exhibits the highest vision performance and excels in non-English languages compared to previous OpenAI models.gpt-4o is engineered for speed and efficiency. Its advanced ability to handle complex queries with minimal resources can translate into cost savings and performance.",
  "card_o3mini_description": "O3-Mini is optimized for efficient processing with reduced parameters. It uses max_completion_tokens instead of max_tokens and requires developer role for interactions. Suitable for lightweight applications requiring quick responses.",
  "card_deepseekr1_description":  "DeepSeek-R1 excels at reasoning tasks using a step-by-step training process, such as language, scientific reasoning, and coding tasks. It features 671B total parameters with 37B active parameters, and 128k context length.DeepSeek-R1 builds on the progress of earlier reasoning-focused models that improved performance by extending Chain-of-Thought (CoT) reasoning. DeepSeek-R1 takes things further by combining reinforcement learning (RL) with fine-tuning on carefully chosen datasets. It evolved from an earlier version, DeepSeek-R1-Zero, which relied solely on RL and showed strong reasoning skills but had issues like hard-to-read outputs and language inconsistencies. To address these limitations, DeepSeek-R1 incorporates a small amount of cold-start data and follows a refined training pipeline that blends reasoning-oriented RL with supervised fine-tuning on curated datasets, resulting in a model that achieves state-of-the-art performance on reasoning benchmarks.",
  "card_gpt4omini_description": "GPT-4o mini enables a broad range of tasks with its low cost and latency, such as applications that chain or parallelize multiple model calls (e.g., calling multiple APIs), pass a large volume of context to the model (e.g., full code base or conversation history), or interact with customers through fast, real-time text responses (e.g., customer support chatbots).Today, GPT-4o mini supports text and vision in the API, with support for text, image, video and audio inputs and outputs coming in the future. The model has a context window of 128K tokens and knowledge up to October 2023. Thanks to the improved tokenizer shared with GPT-4o, handling non-English text is now even more cost effective.",
  "card_ministral3b_description": "Ministral 3B is a state-of-the-art Small Language Model (SLM) optimized for edge computing and on-device applications. As it is designed for low-latency and compute-efficient inference, it it also the perfect model for standard GenAI applications that have real-time requirements and high-volume.",
  "card_mistralsmall_description":  "A small model optimized for low latency. Very efficient for high volume and low latency workloads. Mistral Small is Mistral's smallest proprietary model, it outperforms Mixtral 8x7B and has lower latency.Specialized in RAG. Crucial information is not lost in the middle of long context windows (up to 32K tokens).Strong in coding. Code generation, review and comments. Supports all mainstream coding languages.Multi-lingual by design. Best-in-class performance in French, German, Spanish, and Italian - in addition to English. Dozens of other languages are supported.Responsible AI. Efficient guardrails baked in the model, with additional safety layer with safe_mode option",
  "card_phi35moe_description": "Phi-3.5-MoE is a lightweight, state-of-the-art open model built upon datasets used for Phi-3 - synthetic data and filtered publicly available documents - with a focus on very high-quality, reasoning dense data. The model supports multilingual and comes with 128K context length (in tokens). The model underwent a rigorous enhancement process, incorporating supervised fine-tuning, proximal policy optimization, and direct preference optimization to ensure precise instruction adherence and robust safety measures.",
  "card_phi35mini_description": "Phi-3.5-mini is a lightweight, state-of-the-art open model built upon datasets used for Phi-3 - synthetic data and filtered publicly available websites - with a focus on very high-quality, reasoning dense data. The model belongs to the Phi-3 model family and supports 128K token context length. The model underwent a rigorous enhancement process, incorporating both supervised fine-tuning, proximal policy optimization, and direct preference optimization to ensure precise instruction adherence and robust safety measures.",
  "card_phi35vision_description":  "Phi-3.5-vision is a lightweight, state-of-the-art open multimodal model built upon datasets which include - synthetic data and filtered publicly available websites - with a focus on very high-quality, reasoning dense data both on text and vision. The model belongs to the Phi-3 model family, and the multimodal version comes with 128K context length (in tokens) it can support. The model underwent a rigorous enhancement process, incorporating both supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures.",
  "card_phi3mini128k_description": "The Phi-3-Mini-128K-Instruct is a 3.8 billion-parameter, lightweight, state-of-the-art open model trained using the Phi-3 datasets.This dataset includes both synthetic data and filtered publicly available website data, with an emphasis on high-quality and reasoning-dense properties.After initial training, the model underwent a post-training process that involved supervised fine-tuning and direct preference optimization to enhance its ability to follow instructions and adhere to safety measures.When evaluated against benchmarks that test common sense, language understanding, mathematics, coding, long-term context, and logical reasoning, the Phi-3 Mini-128K-Instruct demonstrated robust and state-of-the-art performance among models with fewer than 13 billion parameters.",
  "card_phi3mini4k_description":  "The Phi-3-Mini-4K-Instruct is a 3.8B parameters, lightweight, state-of-the-art open model trained with the Phi-3 datasets that includes both synthetic data and the filtered publicly available websites data with a focus on high-quality and reasoning dense properties. The model belongs to the Phi-3 family with the Mini version in two variants 4K and 128K which is the context length (in tokens) that it can support.",
  "card_phi4_description": "Phi-4 is a state-of-the-art open model built upon a blend of synthetic datasets, data from filtered public domain websites, and acquired academic books and Q&A datasets. The goal of this approach was to ensure that small capable models were trained with data focused on high quality and advanced reasoning."
}
